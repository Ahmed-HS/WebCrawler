/*
 * This Kotlin source file was generated by the Gradle 'init' task.
 */
package Web.crawler

import ca.rmen.porterstemmer.PorterStemmer
import org.jsoup.Jsoup
import java.io.File
import java.util.*
import java.util.concurrent.Executors
import java.util.concurrent.Semaphore
import java.util.concurrent.atomic.AtomicInteger
import java.util.concurrent.locks.Lock
import java.util.regex.Matcher
import java.util.regex.Pattern

class BasicWebCrawler(val maxCycles: Int) {

    private val visitedLinks: MutableSet<String> = mutableSetOf()

    //private val toVisit:Queue<String> = LinkedList()
    private val stemmer: PorterStemmer = PorterStemmer()
    val invertedIndex: MutableMap<String, MutableSet<Pair<String, Int>>> = sortedMapOf()
    private val stopWords: Set<String> = File("StopWords.txt").readLines().toSet()
    val executor = Executors.newFixedThreadPool(5)
    @Volatile
    private var currentCycle:Int = 0

    private val runningJobs = AtomicInteger(1)

    fun addToIndex(word: String, position: Int, link: String) {
        synchronized(invertedIndex)
        {
            invertedIndex[word] = invertedIndex.getOrDefault(word, mutableSetOf())
                    .apply {
                        add(link to position)
                    }
        }
    }

    fun start(link: String)
    {
        visitedLinks.add(link)
        executor.execute { crawl(link) }
        //countDownLatch.await()
        //executor.awaitTermination(1,TimeUnit.HOURS)
        while (runningJobs.get() > 0) {

        }
        executor.shutdown()
        println("Done")
    }

    private fun crawl(link: String) {


        //1. Fetch the HTML page
        val document = try {
            Jsoup.connect(link).get()
        }
        catch (e: Exception)
        {
            runningJobs.decrementAndGet()
            return
        }

        //2. Parse the HTML to extract the links
        val linksOnPage = document.select("a[href]")

        //3. Get all the text on the page
        val bodyText = document.select("body").text()

        val tokenizer: Pattern = Pattern.compile("[a-zA-Z]+")

        val matcher: Matcher = tokenizer.matcher(bodyText)

        var position: Int = 0

        while (matcher.find()) {
            val word = matcher.group().toLowerCase()
            //println(word)
            val stem = stemmer.stemWord(word)
            //println(stem)

            if (!stopWords.contains(stem)) {
                addToIndex(stem, position, link)
            }

            position++
        }
        //println("Processed link: $link")
        //4. For each extracted URL add it to the toVisit list
        for (newLink in linksOnPage) {
            val linkUrl = newLink.attr("abs:href")

            synchronized(visitedLinks)
            {
                if (visitedLinks.add(linkUrl))
                {
                    synchronized(currentCycle)
                    {
                        if(currentCycle < maxCycles)
                        {
                            currentCycle++
                            runningJobs.incrementAndGet()
                            //println("Enqueued link: $linkUrl")
                            executor.execute {crawl(linkUrl)}
                        }
                    }
                }
            }
        }

        runningJobs.decrementAndGet()

    }
}



fun main(args: Array<String>) {
    val wrrryyy = "https://tender-shannon-bc1412.netlify.app/"
    val cnn = "http://www.cnn.com"
    val google = "http://www.google.com"
    val crawler = BasicWebCrawler(300)
    crawler.start(cnn)

    val result = crawler.invertedIndex

    val outputFile = File("out.txt")
    outputFile.writeText("")

    for ((word, list) in result) {
        outputFile.appendText("$word, frequency: ${list.size} \n")

        for (item in list)
        {
            outputFile.appendText("$item \n")
        }

        outputFile.appendText("------------------------------\n")
        //println("$word - frequency: ${list.size}")
        //println(list)
    }

}
